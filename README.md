# About my first project on GitHub
I am currently learning a basic data engineering learning roadmap and I am on week 9 out of a 16 week program.

For week 9, the focus is on cloud integration. Where I am supposed to extend my existing ETL pipeline by uploading my Parquet dataset into a cloud storage platform such as as AWS S3 or Google Cloud Storage (GCS)

By the end of the project, I should be able to:
1. Create and configure a cloud storage bucket
2. Setup IAM permissions or service accounts for secure access
3. Write a python script to upload Parquet files to the cloud
4. Validate that my data is correctly uploaded and accessible from the cloud console.
